{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo tutorial - youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import speech\n",
    "\n",
    "# we need to instantiate the client\n",
    "client = speech.SpeechClient.from_service_account_file('/home/dani-boy/MIT-DNN_repo/text2speech/config/key.json')\n",
    "\n",
    "\n",
    "# we need to create our file name:\n",
    "file_name = '/home/dani-boy/MIT-DNN_repo/text2speech/data/teste2.mp3'\n",
    "\n",
    "#file_name = '/home/dani-boy/MIT-DNN_repo/text2speech/teste2.mp3'\n",
    "\n",
    "#let´s read our file\n",
    "with open(file_name, 'rb') as f:  # 'rb' is our mode\n",
    "    mp3_data = f.read()\n",
    "    \n",
    "    \n",
    "# Now we need to create our recognition audio\n",
    "audio_file = speech.RecognitionAudio(content=mp3_data)\n",
    "\n",
    "\n",
    "#Now we need to configure the media file output\n",
    "config = speech.RecognitionConfig(\n",
    "        sample_rate_hertz=44100,\n",
    "        enable_automatic_punctuation=True,\n",
    "        language_code='pt-BR'\n",
    ")\n",
    "\n",
    "\n",
    "# Now let´s recognize our audio file or detect speech\n",
    "response = client.recognize(\n",
    "    config=config,\n",
    "    audio=audio_file\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executar o reconhecimento de fala síncrono em um arquivo local\n",
    "\n",
    "Modelo google com alguma mudanças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transcribe_file(speech_file):\n",
    "    \"\"\"Transcribe the given audio file.\"\"\"\n",
    "    from google.cloud import speech\n",
    "    import io\n",
    "\n",
    "    #client = speech.SpeechClient()\n",
    "    client = speech.SpeechClient.from_service_account_file('/home/dani-boy/MIT-DNN_repo/text2speech/config/key.json')\n",
    "\n",
    "\n",
    "    with io.open(speech_file, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "\n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "    \n",
    "    config = speech.RecognitionConfig(\n",
    "        sample_rate_hertz=44100,\n",
    "        enable_automatic_punctuation=True,\n",
    "        language_code=\"pt-BR\",\n",
    "    )\n",
    "\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Each result is for a consecutive portion of the audio. Iterate through\n",
    "    # them to get the transcripts for the entire audio file.\n",
    "    for result in response.results:\n",
    "        # The first alternative is the most likely one for this portion.\n",
    "        print()\n",
    "        print(\"Transcript: {}\".format(result.alternatives[0].transcript))\n",
    "        print()\n",
    "        print(\"Confidence: {}\".format(result.alternatives[0].confidence))\n",
    "        print()\n",
    "        print(\"Words: {}\".format(result.alternatives[0].words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/dani-boy/MIT-DNN_repo/text2speech/data/'\n",
    "\n",
    "file = 'teste10.mp3'\n",
    "target = path + file\n",
    "\n",
    "transcribe_file(target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcrever do microfone \n",
    "\n",
    "Tem que baixar os drivers linux de aúdio (inluindo microfone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from google.cloud import speech\n",
    "\n",
    "import pyaudio\n",
    "from six.moves import queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Audio recording parameters\n",
    "RATE = 16000\n",
    "CHUNK = int(RATE / 10)  # 100ms\n",
    "\n",
    "class MicrophoneStream(object):\n",
    "    \"\"\"Opens a recording stream as a generator yielding the audio chunks.\"\"\"\n",
    "\n",
    "    def __init__(self, rate, chunk):\n",
    "        self._rate = rate\n",
    "        self._chunk = chunk\n",
    "\n",
    "        # Create a thread-safe buffer of audio data\n",
    "        self._buff = queue.Queue()\n",
    "        self.closed = True\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._audio_interface = pyaudio.PyAudio()\n",
    "        self._audio_stream = self._audio_interface.open(\n",
    "            format=pyaudio.paInt16,\n",
    "            # The API currently only supports 1-channel (mono) audio\n",
    "            # https://goo.gl/z757pE\n",
    "            channels=1,\n",
    "            rate=self._rate,\n",
    "            input=True,\n",
    "            frames_per_buffer=self._chunk,\n",
    "            # Run the audio stream asynchronously to fill the buffer object.\n",
    "            # This is necessary so that the input device's buffer doesn't\n",
    "            # overflow while the calling thread makes network requests, etc.\n",
    "            stream_callback=self._fill_buffer,\n",
    "        )\n",
    "\n",
    "        self.closed = False\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self._audio_stream.stop_stream()\n",
    "        self._audio_stream.close()\n",
    "        self.closed = True\n",
    "        # Signal the generator to terminate so that the client's\n",
    "        # streaming_recognize method will not block the process termination.\n",
    "        self._buff.put(None)\n",
    "        self._audio_interface.terminate()\n",
    "\n",
    "    def _fill_buffer(self, in_data, frame_count, time_info, status_flags):\n",
    "        \"\"\"Continuously collect data from the audio stream, into the buffer.\"\"\"\n",
    "        self._buff.put(in_data)\n",
    "        return None, pyaudio.paContinue\n",
    "\n",
    "    def generator(self):\n",
    "        while not self.closed:\n",
    "            # Use a blocking get() to ensure there's at least one chunk of\n",
    "            # data, and stop iteration if the chunk is None, indicating the\n",
    "            # end of the audio stream.\n",
    "            chunk = self._buff.get()\n",
    "            if chunk is None:\n",
    "                return\n",
    "            data = [chunk]\n",
    "\n",
    "            # Now consume whatever other data's still buffered.\n",
    "            while True:\n",
    "                try:\n",
    "                    chunk = self._buff.get(block=False)\n",
    "                    if chunk is None:\n",
    "                        return\n",
    "                    data.append(chunk)\n",
    "                except queue.Empty:\n",
    "                    break\n",
    "\n",
    "            yield b\"\".join(data)\n",
    "\n",
    "def listen_print_loop(responses):\n",
    "    \"\"\"Iterates through server responses and prints them.\n",
    "\n",
    "    The responses passed is a generator that will block until a response\n",
    "    is provided by the server.\n",
    "\n",
    "    Each response may contain multiple results, and each result may contain\n",
    "    multiple alternatives; for details, see https://goo.gl/tjCPAU.  Here we\n",
    "    print only the transcription for the top alternative of the top result.\n",
    "\n",
    "    In this case, responses are provided for interim results as well. If the\n",
    "    response is an interim one, print a line feed at the end of it, to allow\n",
    "    the next result to overwrite it, until the response is a final one. For the\n",
    "    final one, print a newline to preserve the finalized transcription.\n",
    "    \"\"\"\n",
    "    num_chars_printed = 0\n",
    "    for response in responses:\n",
    "        if not response.results:\n",
    "            continue\n",
    "\n",
    "        # The `results` list is consecutive. For streaming, we only care about\n",
    "        # the first result being considered, since once it's `is_final`, it\n",
    "        # moves on to considering the next utterance.\n",
    "        result = response.results[0]\n",
    "        if not result.alternatives:\n",
    "            continue\n",
    "\n",
    "        # Display the transcription of the top alternative.\n",
    "        transcript = result.alternatives[0].transcript\n",
    "\n",
    "        # Display interim results, but with a carriage return at the end of the\n",
    "        # line, so subsequent lines will overwrite them.\n",
    "        #\n",
    "        # If the previous result was longer than this one, we need to print\n",
    "        # some extra spaces to overwrite the previous result\n",
    "        overwrite_chars = \" \" * (num_chars_printed - len(transcript))\n",
    "\n",
    "        if not result.is_final:\n",
    "            sys.stdout.write(transcript + overwrite_chars + \"\\r\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            num_chars_printed = len(transcript)\n",
    "\n",
    "        else:\n",
    "            print(transcript + overwrite_chars)\n",
    "\n",
    "            # Exit recognition if any of the transcribed phrases could be\n",
    "            # one of our keywords.\n",
    "            if re.search(r\"\\b(exit|quit)\\b\", transcript, re.I):\n",
    "                print(\"Exiting..\")\n",
    "                break\n",
    "\n",
    "            num_chars_printed = 0\n",
    "\n",
    "def main():\n",
    "    # See http://g.co/cloud/speech/docs/languages\n",
    "    # for a list of supported languages.\n",
    "    language_code = \"pt-BR\"  # a BCP-47 language tag\n",
    "\n",
    "    #client = speech.SpeechClient()\n",
    "    client = speech.SpeechClient.from_service_account_file('/home/dani-boy/MIT-DNN_repo/text2speech/config/key.json')\n",
    "    \n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=RATE,\n",
    "        language_code=language_code,\n",
    "    )\n",
    "\n",
    "    streaming_config = speech.StreamingRecognitionConfig(\n",
    "        config=config, interim_results=True\n",
    "    )\n",
    "\n",
    "    with MicrophoneStream(RATE, CHUNK) as stream:\n",
    "        audio_generator = stream.generator()\n",
    "        requests = (\n",
    "            speech.StreamingRecognizeRequest(audio_content=content)\n",
    "            for content in audio_generator\n",
    "        )\n",
    "\n",
    "        responses = client.streaming_recognize(streaming_config, requests)\n",
    "\n",
    "        # Now, put the transcription responses to use.\n",
    "        listen_print_loop(responses)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Model whatsapi***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another API you can use for speech-to-text is the Google Cloud Speech-to-Text API. It has a free tier that allows you to transcribe up to 60 minutes of audio per month and provides high-quality transcription results. To use this API, you will need to have a Google Cloud account and set up a project. You can then use the Google Cloud SDK or the REST API to integrate the service into your Python code. Here's an example of how to use the Google Cloud Speech-to-Text API in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import requests\n",
    "import json\n",
    "import io\n",
    "import wave\n",
    "import base64\n",
    "import google.auth\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import speech_v1p1beta1 as speech\n",
    "\n",
    "# Load credentials for the Google Cloud Speech API\n",
    "credentials, project = google.auth.default()\n",
    "creds = service_account.Credentials.from_service_account_file('path/to/credentials.json')\n",
    "\n",
    "# Connect to the WhatsApp chat using the WhatsApp API\n",
    "whatsapp_session = requests.Session()\n",
    "qr_code = whatsapp_session.get('https://api.whatsapp.com/send?phone=<phone-number>')\n",
    "whatsapp_session.post('https://web.whatsapp.com/send?phone=<phone-number>', headers={\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n",
    "    'Referer': 'https://web.whatsapp.com/'\n",
    "})\n",
    "\n",
    "# Fetch the voice message from the chat and save it as an audio file\n",
    "chat_id = '1234567890@c.us'\n",
    "message_id = '0123456789abcdef0123456789abcdef012345'\n",
    "message_url = 'https://web.whatsapp.com/getMessage/' + chat_id + '/' + message_id\n",
    "message = whatsapp_session.get(message_url).json()['message']\n",
    "voice_message_data = message['audioMessage']['url']\n",
    "voice_message_file = io.BytesIO(urllib.request.urlopen(voice_message_data).read())\n",
    "with wave.open('voice_message.wav', 'wb') as f:\n",
    "    f.setnchannels(1)\n",
    "    f.setsampwidth(2)\n",
    "    f.setframerate(16000)\n",
    "    f.writeframes(voice_message_file.getbuffer().tobytes())\n",
    "\n",
    "# Use the Google Cloud Speech API to transcribe the audio file\n",
    "client = speech.SpeechClient(credentials=creds)\n",
    "with io.open('voice_message.wav', 'rb') as audio_file:\n",
    "    content = audio_file.read()\n",
    "audio = speech.RecognitionAudio(content=content)\n",
    "config = speech.RecognitionConfig(\n",
    "    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "    sample_rate_hertz=16000,\n",
    "    language_code='pt-BR'\n",
    ")\n",
    "response = client.recognize(config=config, audio=audio)\n",
    "\n",
    "# Send the transcription back to the WhatsApp chat as a reply\n",
    "text = response.results[0].alternatives[0].transcript\n",
    "response_url = 'https://web.whatsapp.com/send?phone=' + chat_id + '&text=' + urllib.parse.quote(text)\n",
    "response_headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n",
    "    'Referer': 'https://web.whatsapp.com/'\n",
    "}\n",
    "whatsapp_session.post(response_url, headers=response_headers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No google api"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several speech-to-text libraries that you can use in Python to avoid using Google's API. One of the most popular is the SpeechRecognition library. Here is an example code that shows how to use this library to transcribe a WAV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "# obtain path to the WAV file\n",
    "wav_file = \"example.wav\"\n",
    "\n",
    "# initialize the recognizer\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# use the recognizer to open the WAV file\n",
    "with sr.AudioFile(wav_file) as source:\n",
    "    # record the audio data\n",
    "    audio_data = r.record(source)\n",
    "    \n",
    "# transcribe the audio data\n",
    "text = r.recognize_sphinx(audio_data)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an API similar to Twilio or Plivo for processing voice messages from WhatsApp, you will need to follow these general steps:\n",
    "\n",
    "Choose a programming language and framework to build your API. Popular options include Python with Flask or Django, Node.js with Express, and Ruby on Rails.\n",
    "\n",
    "Set up a server to host your API. This can be a cloud-based server on services like AWS or Google Cloud, or a local server on your computer.\n",
    "\n",
    "Define the endpoints for your API, which will allow users to interact with it. For example, you might have an endpoint for uploading a voice message from WhatsApp, an endpoint for processing the voice message and transcribing it to text, and an endpoint for sending the transcribed text to the user.\n",
    "\n",
    "Implement the logic for processing the voice message and transcribing it to text. There are several open-source speech-to-text libraries you can use, such as Mozilla's DeepSpeech or Kaldi.\n",
    "\n",
    "Integrate your API with WhatsApp using the WhatsApp Business API. This will allow you to receive voice messages from WhatsApp and send transcribed text messages back to the user.\n",
    "\n",
    "Test your API thoroughly to ensure it is functioning properly and providing accurate transcriptions.\n",
    "\n",
    "Deploy your API to your chosen server and make it available to users.\n",
    "\n",
    "Note that creating an API like this can be a complex and time-consuming process, and may require significant programming and technical expertise. It may be more practical to use an existing service like Twilio or Plivo, or to work with a developer or development team to build your API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an API using Python with Django and deploy it on Heroku, you can follow these general steps:\n",
    "\n",
    "Set up a Django project and create a new app for your API\n",
    "Define the URL patterns for your API endpoints in the urls.py file\n",
    "Define the logic for your API endpoints in the corresponding view functions\n",
    "Implement the speech-to-text functionality in the view functions using a speech-to-text library or service of your choice\n",
    "Set up a Heroku account and install the Heroku CLI\n",
    "Create a new Heroku app and link it to your project using the Heroku CLI\n",
    "Add the required dependencies and configurations to your project, including the Procfile and requirements.txt files\n",
    "Push your code to the Heroku app using the Heroku CLI\n",
    "Test your API by sending requests to the appropriate endpoints.\n",
    "Here are some resources to help you get started:\n",
    "\n",
    "Django documentation: https://docs.djangoproject.com/en/3.2/\n",
    "Heroku documentation: https://devcenter.heroku.com/categories/reference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformerHarv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f70c4599dfa26e3134c71d5878a4845e1d6fff666cf32418c2c5a476336c336"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
